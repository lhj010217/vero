# sw-security-web-app_AI



# 시스템 아키텍처
![image](https://github.com/user-attachments/assets/57978b28-f8a6-4321-94be-d7dec66624fc)


# 모델 아키텍처
![image](https://github.com/user-attachments/assets/c3862302-9bd5-4b1b-b0eb-57f1bfe6a2c4)


## 소개
해당 프로젝트는 개인정보를 포함한 유해한 프롬프트를 필터링하는 AI 모델을 생성하고, API 수준의 접근에서 추론 및 미세 조정을 가능하게 하는 시스템입니다. 데이터 처리, 모델 학습, 차분 프라이버시(Differential Privacy, DP) 적용 등 다양한 과정을 통해 AI 모델의 보안성을 높이고, 악의적인 사용자로부터 시스템을 보호합니다.

## 데이터 처리 과정
학습에 사용되는 데이터는 **MarianMT** 모델을 통해 **영어**로 번역됩니다. 현재 지원되는 언어는 **영어, 한국어, 일본어, 중국어, 불어, 에스파냐어**이며, 향후 추가 언어를 지원할 예정입니다. 번역된 데이터는 **Nlpaug**를 사용하여 다음과 같은 데이터 증강 작업을 거칩니다:

- 동의어 치환
- 단어 삭제
- 단어 교환
- 타이포그래피 변형

이 과정을 통해 악의적인 사용자가 프롬프트를 변형하여 AI 모델을 교란하는 시도를 방지할 수 있습니다.

## 베이스 모델 생성 과정
### 학습 데이터
학습에 사용되는 데이터는 두 가지 주요 데이터셋에서 추출됩니다:
1. **개인정보 유출 사례** (Kaggle PII Dataset)에서 추출한 개인정보가 포함된 프롬프트 10,407개
2. **일반적인 프롬프트 데이터** (Alpaca 모델 학습 데이터 중 랜덤 샘플링) 10,407개

각각의 프롬프트는 라벨링을 통해 학습 데이터로 준비되며, 데이터 정제 및 증강 작업을 거칩니다.

### 모델 학습
사전 학습된 **BERT Classifier** 모델을 사용하여, 위의 데이터를 기반으로 **미세조정(fine-tuning)**을 수행합니다. 모델 학습은 **train/test** 데이터 분할 80% : 20%로 진행되며, 미세조정 결과는 다음과 같습니다:

- **정확도 (accuracy)**: 96.90%
- **정밀도 (precision)**: 99.28%
- **재현율 (recall)**: 94.41%
- **F1-score**: 96.79%

### 차분 프라이버시 적용
모델 학습 과정에서 **Opacus** 라이브러리를 사용하여 **차분 프라이버시(Differential Privacy, DP)**를 적용합니다. 이는 데이터에 약간의 노이즈를 추가하여, AI 모델이 학습하는 데이터에 변형을 일으키며, 해커가 모델의 내부 구조를 예측하는 것을 어렵게 만들어 보안성을 높입니다.

## 베이스 모델 활용
이렇게 생성된 **베이스 모델**은 일반 사용자가 자신의 프롬프트를 확인하는 데 사용됩니다. 또한, 기업에서는 이 모델을 기반으로 새로운 AI를 생성하여, 해당 기업의 관계자들이 사용할 수 있도록 합니다. 학습에 사용되는 데이터는 동일한 처리과정을 거치며, **차분 프라이버시**가 적용되어 기업 내부 데이터를 모델이 직접 학습하는 것을 방지합니다.

## 주요 기술 스택
- **MarianMT**: Transformer 기반 다국어 번역 모델
- **Nlpaug**: 자연어 데이터 증강 라이브러리
- **BERT Classifier**: 사전 학습된 BERT 기반 분류 모델
- **Opacus**: 차분 프라이버시 라이브러리
- **Python**: 기본 언어
- **TensorFlow / PyTorch**: 딥러닝 프레임워크
- **Uvicorn**: ASGI 서버
- **FastAPI**: 웹 프레임워크
- **AWS**: 클라우드 서비스 및 배포
- **Docker**: 애플리케이션 컨테이너화 및 배포

## 개발자 정보
- **개발자**: 이형준
- **소속**: 동국대학교 컴퓨터공학과, Aritificial Intelligence Lab.[http://ailab.dongguk.edu/]
- **연락처**: lhj71485750@gmail.com
